\chapter{Intelligenza Artificiale - Avv. Vincenzi}

Perché parlare di intelligenza artificiale? 
Conoscerla consente di stabilire sistemi informatici solidi (\textit{legal by design}):
\begin{itemize}
    \item Compliant con le normative, riducendo il rischio di sanzioni
    \item Prevenire problemi
    \item Chiarire le responsabilità
\end{itemize}

Oggi si parla tanto di AI a causa del continuo miglioramento delle macchine (legge di Moore) legato anche alla maggiore disponibilità di dati e al continuo sviluppo di algoritmi più efficaci. 
L'intelligenza artificiale generalizza per prevedere ma per far ciò richiede la presenza di un numero di dati molto alto al fine di modellizzare in termini matematici/tecnici il principio di saggezza della folla. 

\section{Definizione di AI}
La Commissione Europea nel 2018 definisce l'intelligenza artificiale nel seguente modo in termini legali:
\begin{quote}
    ``L'Intelligenza Artificiale si riferisce a sistemi che mostrano comportamenti intelligenti analizzando il loro ambiente e intraprendendo azioni - con un certo grado di autonomia - per raggiungere obiettivi specifici.''
\end{quote}
Possiamo definire inoltre due tipi di intelligenze:
\begin{itemize}
    \item Intelligenza debole: effettivamente esistente al giorno d'oggi; un'AI è in grado di svolgere bene il compito per cui è stata programmata. 
    \item Intelligenza forte: \textit{forse} ci si arriverà nel futuro; pone quesiti relativamente alla natura dell'AI come persona o non persona, al fatto che sia in grado di pensare autonomamente, di soffrire, di avere coscienza del proprio io. 
\end{itemize}
L'intelligenza artificiale debole produce il paradosso di Moradec: ragionamenti di alto livello richiedono pochissimo calcolo, ma le capacità sensomotorie di basso livello richiedono enormi risorse computazionali. 
Questo è il motivo per cui attualmente le AI spesso non sono in grado di muoversi nello spazio come esseri umani.

\subsection{Autonomia}
Nei sistemi ``tradizionali'' siamo abituati a ragionare in funzione del determinismo sottostante: a un input corrisponde un output (\textit{if this then that}). 
In ambito di AI non si ha determinismo ma \textit{autonomia}, cosa che per un giurista genera notevoli problemi perché consente di passare da un sistema sicuro e prevedibile a un sistema in cui si ha imprevedibilità e probabilità. 

L'autonomia porta con sé la probabilità, quindi l'imprevedibilità. Se un sistema è imprevedibile possiamo dire che esso decide da solo e se non si tratta di un essere umano che prende le decisioni allora è necessario definire una nuova forma di soggettività giuridica diversa da quelle attuali. 

La definizione di una nuova soggettività giuridica per le AI è fondamentale perché a livello contrattuale è necessario gestire il fatto che lo strumento possa adottare soluzioni imprevedibili. 

Nei contratti c'è un principio di buona fede precontrattuale; se parliamo di soluzioni innovative come l'AI allora dobbiamo essere estremamente trasparenti a livello contrattuale perché molti clienti possono avere aspettative troppo alte rispetto a quello che l'AI è effettivamente in grado di fare.

Per il Parlamento Europeo
\begin{quote}
    ``più i robot sono autonomi, meno possono essere considerati come meri strumenti nelle mani di altri attori (quali il fabbricante, l'operatore, il proprietario, l'utilizzatore, ecc.)''
\end{quote}
e 
\begin{quote}
    ``in ultima analisi, l'autonomia dei robot solleva la questione della loro natura alla luce delle categorie giuridiche esistenti e dell'eventuale necessità di creare una nuova categoria con caratteristiche specifiche e implicazioni proprie''
\end{quote}
Ne conseguono due opinioni:
\begin{itemize}
    \item AI RESPONSABILI perché è difficile capire di chi è la colpa ma le AI sono comunque autonome
    \item AI NON RESPONSABILI perché non sono soggetti di diritto e perché è un azzardo morale (se riconosciamo responsabilità alle AI i costruttori potrebbero scaricare la colpa sulle macchine e nessuno verrebbe più punito; si perderebbe l'efficacia deterrente delle sanzioni perché la colpa sarebbe scaricabile su una macchina anziché sull'uomo)
\end{itemize}
In entrambi i casi l'autonomia implica un dilemma etico-morale: per es. se un'automobile self-driving investe una persona di chi è la colpa?

Oggi non siamo a livelli di sviluppo tali da considerare le macchine nemmeno senzienti, però è uno degli scenari futuri possibili da tenere in considerazione. 

\subsection{Machine learning}
Sono modelli di apprendimento automatizzato nei quali non si dice alla AI cosa fare - come accade nei programmi tradizionali - ma la si programma appunto per apprendere.

Sia consentito un esempio: si supponga che l’obiettivo sia capire quali tipi di bicchieri, cadendo, si possono rompere. 
Un programma tradizionale richiede una precisa indicazione di quali, per esempio quelli di vetro o ceramica, cadendo si romperanno, e di quali invece reggeranno l’urto integri. 
A tale logica si contrappone quella induttiva del machine learning che osserverà la caduta dei bicchieri e - secondo l’impostazione di apprendimento contenuta nel programma - imparerà quali si rompono e quali no.

In sostanza il principio su cui si basa l'intelligenza artificiale è l'osservazione e l'apprendimento oppure la generalizzazione al fine di fare previsioni. 

Se generalizzo per prevedere allora introduco nel sistema una grande componente probabilistica, che dal punto di vista giuridico è notevolmente diversa dalla certezza.

Il machine learning è una questione importante per il diritto perché se i dati sono sbagliati possono creare enormi problemi, dal momento che se lo strumento osserva dati sbagliati può trarre conclusioni che compromettono l'intero sistema e la sua evoluzione.
Questo effetto è legato al fatto che AI con dati diversi producono output diversi ed evolvono in modo diverso. 

Si tratta dunque di un problema qualitativo oltre che quantitativo relativamente ai dati forniti.

\subsection{Deep Learning}
Si tratta di modelli di apprendimento ispirati alla struttura del cervello biologico che si basano su reti neurali artificiali. 
In tal caso il percorso input-output è il risultato del percorso che, attraverso i vari livelli della rete neurale, viene seguito dal dato in entrata. 
Ogni livello infatti svilupperà un microprocesso input/output che comunicherà al livello ad esso successivo, fino all’esito finale.

Il deep learning porta con sé il grande problema relativo all'\textit{opacità}: il sistema viene visto come una black box quindi se ne conoscono l'input e l'output ma non si può fare reverse engineering sul funzionamento interno che porta da uno all'altro (si tratta anche di metodi per impedire la rivelazione di un segreto industriale). 

\section{Responsabilità e AI}
Chi sbaglia se l'AI non fa quello che deve fare? Il programmatore? L'utilizzatore? L'operatore? Il formatore o data scientist (chi seleziona i dati e li supervisiona)?

Dato che oggi la responsabilità è un problema irrisolvibile o comunque irrisolto, dato che bisognerebbe analizzare caso per caso le applicazioni dell'AI (che però è general purpose quindi analizzarne tutte le applicazioni sarebbe impensabile), si rimedia temporaneamente applicando un principio di \textit{Responsabilità Sussidiaria}.

\subsection{Responsabilità sussidiaria}
Si manifesta in una di queste forme:
\begin{itemize}
    \item Forme di responsabilità oggettiva, come quella del produttore, del genitore o del padrone di un animale
    \item Assicurazione obbligatoria
    \item Fondi di risarcimento
\end{itemize} 
Fondamentalmente nell'entrare nel mondo dell'AI bisogna o assumere una responsabilità oggettiva oppure essere coperti da assicurazione o prendere parte a un fondo di risarcimento.

La responsabilità sussidiaria subentra laddove non sia chiaro in sede di prima analisi chi sia il responsabile.

Oggi il Robot (o comunque l’AI), non avendo capacità di intendere e di volere, non può esser annoverato nell’elenco dei soggetti imputabili, o meglio nell’elenco dei soggetti tout court, potendo esser quindi al più lo strumento con cui viene commesso il reato dalla persona che - secondo le logiche classiche del diritto penale - viene ritenuta
responsabile.

Hallevy, partendo dalla considerazione che una AI potrebbe avere in alcuni casi una capacità pensante (e quindi di intendere e di volere) e che - in ogni caso - gli ordinamenti giuridici moderni stanno producendo forme di responsabilità penale anche per le persone giuridiche (e quindi non solo per le persone fisiche), vi possono esser tre livelli di coinvolgimento della AI in un reato:\
\begin{enumerate}
    \item AI potrebbe esser utilizzata consapevolmente per commettere un reato, ed in tal caso figurerebbe come agente innocente (alla stregua di un bambino) e si applicherebbe la responsabilità al soggetto che tramite essa ha commesso il reato. La particolarità della formulazione sta nel vedere la AI non come strumento-oggetto, ma come soggetto non imputabile. In tale ipotesi del reato risponde chi ha determinato (volontariamente) la AI a commetterlo.
    \item caso in cui il reato sia commesso da una AI durante il funzionamento ordinario della stessa. Ancora una volta il paradigma è quello dell’agente innocente, ma il soggetto determinante risponderà solo se il reato è punibile a titolo di colpa, non essendo la commissione del reato dallo stesso voluta ed essendosi verificato solo per negligenza, imprudenza o imperizia nella programmazione o utilizzo del sistema.
    \item Hallevy riconosce in capo alla AI non solo la possibilità di attribuirle la condotta, ma anche quella di riconoscerne una \textit{mens rea}, ossia l’elemento soggettivo. In tal caso, quindi, oltre o in alternativa alla responsabilità dell’essere umano, Hallevy ritiene ipotizzabile la responsabilità della macchina proprio in quanto centro di imputazione sia della condotta che del correlato animus.
\end{enumerate}

\section{Diritti d'autore e AI}
L'intelligenza artificiale oggi produce contenuti nuovi, affiancando una propria forma di creatività a quella dell'uomo. 
